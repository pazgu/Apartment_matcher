{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e28a4b4a",
   "metadata": {},
   "source": [
    "# Scraping madlan.co.il"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83f890b",
   "metadata": {},
   "source": [
    "In this file we show scraping of madlan.co.il website for the apartments in Tel Aviv district.\n",
    "You can adjust the code to scrape different city's pages.\n",
    "In our project, we scraped the pages of Tel Aviv, Haifa, and Jerusalem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d6f351",
   "metadata": {},
   "source": [
    "## Using BeautifulSoup "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df839fbb",
   "metadata": {},
   "source": [
    "Functions to scrape the specific page html with BeautifulSoup to json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "415f6346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def append_to_json(data, filename):\n",
    "    try:\n",
    "        with open(filename, 'r',encoding='utf-8') as file:\n",
    "            file_data = json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        file_data = []\n",
    "    \n",
    "    file_data.append(data)\n",
    "    \n",
    "    with open(filename, 'w',encoding='utf-8') as file:\n",
    "        json.dump(file_data, file, indent=4,ensure_ascii=False)\n",
    "        \n",
    "\n",
    "def scrape_page(url,page_number,headers):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser',from_encoding='utf-8')\n",
    "        \n",
    "        return {'page': page_number, 'source': str(soup()), 'timestamp': datetime.now().isoformat()}\n",
    "    else:\n",
    "        raise Exception(f\"Failed to fetch page {page_number}, status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb32ceda",
   "metadata": {},
   "source": [
    "## Getting data - using for every 15-30 pages different ip from vpn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00765f37",
   "metadata": {},
   "source": [
    "### Extracting apartments data - Buy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c50ba65",
   "metadata": {},
   "source": [
    "in order to scrape the madlan.co.il pages, you need to use different user-agents, time.sleep() , and ip addresses every 15-30 pages because of bot detection in the website. Here, we used Proton VPN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97b4dd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 136 scraped successfully.\n",
      "Page 137 scraped successfully.\n",
      "Page 138 scraped successfully.\n",
      "Page 139 scraped successfully.\n",
      "Page 140 scraped successfully.\n",
      "Page 141 scraped successfully.\n",
      "Page 142 scraped successfully.\n",
      "Page 143 scraped successfully.\n",
      "Page 144 scraped successfully.\n",
      "Page 145 scraped successfully.\n",
      "Page 146 scraped successfully.\n",
      "Page 147 scraped successfully.\n",
      "Page 148 scraped successfully.\n",
      "Page 149 scraped successfully.\n",
      "Page 150 scraped successfully.\n",
      "Page 151 scraped successfully.\n",
      "Page 152 scraped successfully.\n",
      "Page 153 scraped successfully.\n",
      "Page 154 scraped successfully.\n",
      "Page 155 scraped successfully.\n",
      "Page 156 scraped successfully.\n",
      "Page 157 scraped successfully.\n",
      "Page 158 scraped successfully.\n",
      "Page 159 scraped successfully.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    output_file = \"madlan_scraped_pages1.json\"\n",
    "    headers_lst = [\n",
    "        {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\"},\n",
    "        {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\"},\n",
    "        {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"},\n",
    "        {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"},\n",
    "        {\"User-Agent\":\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"},\n",
    "        {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15\"},\n",
    "        {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15\"},\n",
    "        {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\" }       \n",
    "    ]\n",
    "    headers = random.choice(headers_lst)\n",
    "    \n",
    "    #URL:\n",
    "    domain = \"https://www.madlan.co.il\"\n",
    "    sub_domain = \"for-sale\"\n",
    "    city = \"תל-אביב-יפו-ישראל\"\n",
    "    \n",
    "    for page_number in range(136, 160): # adjust the range from 1-15, 16-30 and so on.\n",
    "        start_page = page_number\n",
    "        page = f\"page={start_page}\" if start_page>1 else \"\"\n",
    "        \n",
    "        #filtering the results in madlan\n",
    "        filters = \"filters=_0-10000000___agent%2Cprivate_______0-100000_____&sort=date-desc&tracking_search_source=filter_apply&marketplace=residential\"\n",
    "        url = f\"{domain}/{sub_domain}/{city}?{page}&{filters}\"\n",
    "        try:\n",
    "            page_data = scrape_page(url,page_number,headers) #scraping the page\n",
    "            append_to_json(page_data, output_file) #adding the page to json format\n",
    "            print(f\"Page {page_number} scraped successfully.\")\n",
    "            time.sleep(3.43)  # Delay to avoid being blocked\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9807c137",
   "metadata": {},
   "source": [
    "### Extracting apartment data - Rent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b5218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    output_file = \"madlan_scraped_pages2.json\"\n",
    "    headers_lst = [\n",
    "        {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\"},\n",
    "        {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\"},\n",
    "        {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"},\n",
    "        {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"},\n",
    "        {\"User-Agent\":\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"},\n",
    "        {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15\"},\n",
    "        {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15\"},\n",
    "        {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\" }       \n",
    "    ]\n",
    "    headers = random.choice(headers_lst)\n",
    "    \n",
    "    #URL:\n",
    "    domain = \"https://www.madlan.co.il\"\n",
    "    sub_domain = \"for-rent\"\n",
    "    city = \"תל-אביב-יפו-ישראל\"\n",
    "    \n",
    "    for page_number in range(1, 70): #same as the previous - change range 1-15, 16-30 and so on..\n",
    "        start_page = page_number\n",
    "        page = f\"page={start_page}\" if start_page>1 else \"\"\n",
    "        filters = \"filters=_0-15000__________0-10000_____&sort=date-desc&tracking_search_source=filter_apply&tracking_event_source=list_regular_card&tracking_list_index=1&marketplace=residential\"\n",
    "        url = f\"{domain}/{sub_domain}/{city}?{page}&{filters}\"\n",
    "        try:\n",
    "            page_data = scrape_page(url,page_number,headers)\n",
    "            append_to_json(page_data, output_file)\n",
    "            print(f\"Page {page_number} scraped successfully.\")\n",
    "            time.sleep(3.43)  # Delay to avoid being blocked\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dc6660",
   "metadata": {},
   "source": [
    "### Combine to one JSON file: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809742d3",
   "metadata": {},
   "source": [
    "Create a combined json file that has all html page sources of apartment to buy, and apartments to rent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f30e67fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = r\"madlan_scraped_pages1.json\"\n",
    "with open(path1, 'r', encoding='utf-8-sig') as file:\n",
    "    buys = json.load(file)\n",
    "\n",
    "buys = {\n",
    "    \"deal_type\":\"buys\",\n",
    "    \"info\":buys\n",
    "}\n",
    "    \n",
    "path2 = r\"madlan_scraped_pages2.json\"\n",
    "with open(path2, 'r', encoding='utf-8-sig') as file:\n",
    "    rents = json.load(file)\n",
    "\n",
    "rents = {\n",
    "    \"deal_type\":\"rents\",\n",
    "    \"info\":rents\n",
    "}\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"data\":[buys,rents]\n",
    "}\n",
    "\n",
    "\n",
    "path3 = r\"madlan_scraped_pages_all.json\"\n",
    "with open(path3, 'w', encoding='utf-8-sig') as file:\n",
    "    json.dump(data, file, indent=4,ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
